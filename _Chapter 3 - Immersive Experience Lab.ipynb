{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6b407c-6b2f-4ab5-ac17-463217050921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/Cassini-chris/IEL IEL3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3002462d-e40b-498e-9630-fa2fd2024955",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Welcome Message\n",
    "from IPython.display import Image\n",
    "Image(\"img/head.png\", width=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41da48c-60d3-477b-bc41-179c1f618ad5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Guiding Information\n",
    "Image(\"img/task_0.png\", width=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6cdec8-21f0-4c93-9dd3-395168b4ba9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting the Machine Type and GPU\n",
    "# -----------------------------------\n",
    "# Task 1: Install Watermark and retrieve system architecture by running the code below\n",
    "\n",
    "!pip install watermark \n",
    "!pip install watermark[gpu]\n",
    "%load_ext watermark\n",
    "%watermark\n",
    "%watermark --iversions\n",
    "%watermark --gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea85ee5-3b4f-4b7d-862b-88dc100a55cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Process architecture\n",
    "Image(\"img/task_8a.png\", width=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd91660-b4dd-4f52-b61e-fb91de4b645c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"img/task_8b.png\", width=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8248beb3-6c0a-470a-b12b-4b75f5b4a853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boostrapping the environment \n",
    "Image(\"img/task_1.png\", width=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f39e97-ae59-40ea-8d49-3359a0ff7aaf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Boostrapping the environment\n",
    "# ----------------------------\n",
    "# Task 2: Run the cell and fill in the parameter values\n",
    "\n",
    "project = !gcloud config get-value project\n",
    "PROJECT_ID = project[0]\n",
    "REGION = input('Enter the name of the Region: ')\n",
    "GCS_BUCKET = input('Enter the name of the GCS bucket: ')\n",
    "source_document = input('Enter the name of the source document: ')\n",
    "print (\"Your PROJECT_ID is: \", PROJECT_ID)\n",
    "\n",
    "\n",
    "### ANSWER ############################################################\n",
    "# project = !gcloud config get-value project\n",
    "# PROJECT_ID = project[0]\n",
    "# REGION = 'us-central1'\n",
    "# GCS_BUCKET = 'cymbalstay-docs-iel-genai-dryrun'\n",
    "# source_document = 'full_cymbal_stay.pdf'\n",
    "# PROJECT_ID\n",
    "#######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3faf7722-2400-4b05-aea9-8bf10be76cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "Image(\"img/task_3.png\", width=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9f9da7-32ae-4913-8ef8-d13b37755404",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "# -----------------------------------\n",
    "# Task 3: Find the Packages on PyPi and install them accordingly using \"!pip install\"\n",
    "# URL: https://pypi.org/\n",
    "\n",
    "!pip install \"Package Name: AI Platform\"\n",
    "!pip install \"Package Name: DocAI\"\n",
    "!pip install \"Package Name: ScaNN\"\n",
    "!pip install \"Package Name: PyPDF\"\n",
    "\n",
    "print('Installation completed!')\n",
    "\n",
    "### ANSWER ############################################################\n",
    "# !pip install google.cloud.aiplatform\n",
    "# !pip install google-cloud-documentai\n",
    "# !pip install scann\n",
    "# !pip install PyPDF2\n",
    "#######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0fce74-14ee-45bf-8630-e8ca1971cf6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importing packages\n",
    "# -----------------------------------\n",
    "# Task 4: Just run this cell. There is no todo!\n",
    "import os\n",
    "import io\n",
    "import json\n",
    "import base64\n",
    "import requests\n",
    "import concurrent.futures\n",
    "import time\n",
    "import PyPDF2\n",
    "import IPython\n",
    "import PIL\n",
    "import PIL.ImageFont, PIL.Image, PIL.ImageDraw\n",
    "import shapely\n",
    "import scann\n",
    "import numpy as np\n",
    "\n",
    "import vertexai.preview.language_models\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud import documentai\n",
    "from google.cloud.documentai_v1 import Document\n",
    "from google.cloud import storage\n",
    "\n",
    "print(\"AI-Platform Version. Make sure version is above 1.30. Your Version: \", aiplatform.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94865b03-0bac-4ce0-82b4-abed276b7a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate VertexAI | DocAI clients \n",
    "Image(\"img/task_5.png\", width=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547e373b-4df6-47ba-9184-e9eeecab8c41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initiate VertexAI clients \n",
    "# -----------------------------------\n",
    "# Task 5a: Just run this cell. There is no todo!\n",
    "\n",
    "vertexai.init(project = PROJECT_ID, location = REGION)\n",
    "aiplatform.init(project = PROJECT_ID, location = REGION)\n",
    "\n",
    "print(\"Full Region name: \" , REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46eb928-78b2-4895-ba53-1fcb46129eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate Document AI client\n",
    "# -----------------------------------\n",
    "# Task 5b: Add your Region Prefix to the DocAI API Endpoint via client_options\n",
    "# For example: us-central1-a = us | europe-west6-c = europe\n",
    "\n",
    "# Set Region Prefix\n",
    "REGION_PREFIX = \"Add Region Prefix here\"\n",
    "print(REGION_PREFIX)\n",
    "\n",
    "### ANSWER ############################################################\n",
    "# REGION_PREFIX = REGION.split('-')[0]\n",
    "# print(REGION_PREFIX)\n",
    "######################################################################\n",
    "\n",
    "docai_client = documentai.DocumentProcessorServiceClient(\n",
    "    client_options = dict(api_endpoint = f\"{REGION_PREFIX}-documentai.googleapis.com\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f816092d-188d-4bd3-9872-ae456dff9f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Models\n",
    "Image(\"img/task_6.png\", width=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db1b448-448b-4f2a-9ecb-4efb03f47c1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load Models\n",
    "# -----------------------------------\n",
    "# Task 6: Go to https://cloud.google.com/vertex-ai/docs/generative-ai/learn/models\n",
    "# Find the latest text model and the embeedings model from June 2023 and add them to the code below:\n",
    "\n",
    "embedding_model = vertexai.preview.language_models.TextEmbeddingModel.from_pretrained('LATEST Embedding Model')\n",
    "textgen_model = vertexai.preview.language_models.TextGenerationModel.from_pretrained('USE Text Model Version from June 2023')\n",
    "\n",
    "### ANSWER ############################################################\n",
    "# embedding_model = vertexai.preview.language_models.TextEmbeddingModel.from_pretrained('textembedding-gecko@001')\n",
    "# textgen_model = vertexai.preview.language_models.TextGenerationModel.from_pretrained('text-bison@001')\n",
    "######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a073780-470d-4640-8467-1e7c4a145e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "Image(\"img/task_7.png\", width=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec41d63f-f48d-403f-8960-1ee671864e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract_pdf_text\n",
    "# -----------------------------------\n",
    "# Task 7: Go to https://cloud.google.com/document-ai/docs/form-parser\n",
    "# Find the latest form parser and add it to the Document AI processor below in the variable 'PARSER_VERSION':\n",
    "\n",
    "def extract_pdf_text():\n",
    "    \"\"\"\n",
    "    Extracts text from PDF documents using Google Cloud Document AI.\n",
    "\n",
    "    Args:\n",
    "        source_document (str): The path to the PDF document to extract text from.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, where each dictionary represents a paragraph or table in the PDF document.\n",
    "        Each dictionary contains the following keys:\n",
    "            * page_content (str): The text of the paragraph or table.\n",
    "            * metadata (dict): Metadata about the paragraph or table, including the page number, paragraph or table number,\n",
    "            and the VME ID (a unique identifier for the paragraph or table).\n",
    "            * extras (dict): Optional additional information about the paragraph or table, such as the vertices of the\n",
    "            bounding box.\n",
    "    \"\"\"\n",
    "    # Initialize Google Cloud Storage client\n",
    "    gcs = storage.Client(project=PROJECT_ID)\n",
    "\n",
    "    # Get the PDF document from the specified bucket\n",
    "    bucket = gcs.bucket(GCS_BUCKET)\n",
    "    blob = bucket.blob(source_document)\n",
    "    response = blob.download_as_bytes()\n",
    "\n",
    "    # Read the PDF document as a PDF object\n",
    "    pdf = PyPDF2.PdfReader(io.BytesIO(response))\n",
    "  \n",
    "     # Extract text from each page of the PDF document\n",
    "    pdfs = []\n",
    "    for page_num, page in enumerate(pdf.pages, 1):\n",
    "        # Create a new PDFWriter object\n",
    "        writer = PyPDF2.PdfWriter()\n",
    "\n",
    "        # Add the current page to the PDFWriter object\n",
    "        writer.add_page(page)\n",
    "\n",
    "        # Write the contents of the PDFWriter object to a BytesIO object\n",
    "        with io.BytesIO() as bytes_stream:\n",
    "                pdfs.append(writer.write(bytes_stream)[1].getbuffer().tobytes())\n",
    "\n",
    "                \n",
    "     # Initialize Google Cloud Document AI client\n",
    "    docfai_client = documentai.DocumentProcessorServiceClient()\n",
    "\n",
    "    # Define parameters for Document AI processor\n",
    "    PARSER_DISPLAY_NAME = \"my_general_processor\"\n",
    "    PARSER_TYPE = \"FORM_PARSER_PROCESSOR\"\n",
    "    \n",
    "    PARSER_VERSION = \"---- ADD DOCUMENT PARSER HERE ---- \"\n",
    "        \n",
    "    ### ANSWER ############################################################\n",
    "    # PARSER_VERSION = \"pretrained-form-parser-v2.1-2023-06-26\" \n",
    "    ######################################################################   \n",
    "   \n",
    "\n",
    "    # Get the Document AI processor\n",
    "    parser = None\n",
    "    for p in docai_client.list_processors(parent=f\"projects/{PROJECT_ID}/locations/{REGION_PREFIX}\"):\n",
    "        if p.display_name == PARSER_DISPLAY_NAME:\n",
    "            parser = p\n",
    "            break               \n",
    "                \n",
    "    if parser is None:\n",
    "        # Create a new Document AI processor if it doesn't exist\n",
    "        parser = docai_client.create_processor(\n",
    "            parent=f\"projects/{PROJECT_ID}/locations/{REGION_PREFIX}\",\n",
    "            processor=dict(\n",
    "                display_name=PARSER_DISPLAY_NAME, type_=PARSER_TYPE, default_processor_version=PARSER_VERSION\n",
    "            ),\n",
    "        )\n",
    "        \n",
    "    # Set the rate limit for processing PDF documents\n",
    "    rate_limit_minute = 120\n",
    "    adjust_rate_limit = rate_limit_minute / 2\n",
    "    \n",
    "\n",
    "    def docai_runner(p, start, raw_document):\n",
    "      process_options = documentai.ProcessOptions(\n",
    "      individual_page_selector=documentai.ProcessOptions.IndividualPageSelector(\n",
    "          pages=[1] # e.g. in case you only want to process individual pages\n",
    "      )\n",
    "      )\n",
    "      request = documentai.ProcessRequest(\n",
    "            name=parser.name,\n",
    "            raw_document=raw_document,\n",
    "            process_options=process_options,\n",
    "        )     \n",
    "      sleep_time = (p * (60/adjust_rate_limit)) - (time.time() - start)\n",
    "      if sleep_time > 0: time.sleep(sleep_time)\n",
    "      return (p, docai_client.process_document(request = dict(raw_document = raw_document, name = parser.name)))\n",
    "    print(f\"The expected runtime for PDF text extraction is {(len(pdfs)/adjust_rate_limit):.2f} minutes\")\n",
    "    results = [None] * len(pdfs)\n",
    "    start = time.time()\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers = len(pdfs)) as executor:\n",
    "        futures = [\n",
    "            executor.submit(\n",
    "                docai_runner,\n",
    "                p, start,\n",
    "                documentai.RawDocument(content = pdf, mime_type = 'application/pdf')\n",
    "            ) for p, pdf in enumerate(pdfs)\n",
    "        ]\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "          results[future.result()[0]] = (Document.to_dict(future.result()[1].document))\n",
    "    len(pdfs), len(results)\n",
    "    documents = []\n",
    "    page_images = []\n",
    "    for r, result in enumerate(results):\n",
    "        results[r]['metadata'] = dict(vme_id = str(r))\n",
    "        document_image = PIL.Image.open(\n",
    "            io.BytesIO(\n",
    "                base64.decodebytes(result['pages'][0]['image']['content'].encode('utf-8'))\n",
    "                )\n",
    "            )\n",
    "        page_images.append(document_image)\n",
    "        tables = []\n",
    "        for t, table in enumerate(result['pages'][0]['tables']):\n",
    "          table_txt = ''\n",
    "          if 'text_anchor' in table['layout'].keys():\n",
    "            for s, segment in enumerate(table['layout']['text_anchor']['text_segments']):\n",
    "              if t == 0 and s == 0: start = 0\n",
    "              else: start = int(segment['start_index'])\n",
    "              end = int(segment['end_index'])\n",
    "              table_txt += result['text'][start:end+t]\n",
    "              # print(table_txt)\n",
    "          documents.append(\n",
    "              dict(\n",
    "                  page_content = table_txt,\n",
    "                  metadata = dict(\n",
    "                      page = r+1,\n",
    "                      table = t+1,\n",
    "                      vme_id = str(len(documents)),\n",
    "                      filename = source_document.split('/')[-1],\n",
    "                      source_document = source_document\n",
    "                  ),\n",
    "                  extras = dict(\n",
    "                      vertices = vertices\n",
    "                  )\n",
    "              )\n",
    "          )\n",
    "        for p, paragraph in enumerate(result['pages'][0]['paragraphs']):\n",
    "          paragraph_txt = ''\n",
    "          for s, segment in enumerate(paragraph['layout']['text_anchor']['text_segments']):\n",
    "            if p == 0 and s == 0: start = 0\n",
    "            else: start = int(segment['start_index'])\n",
    "            end = int(segment['end_index'])\n",
    "            paragraph_txt += result['text'][start:end+1]\n",
    "            #print(paragraph_txt)\n",
    "          vertices = []\n",
    "          for vertex in paragraph['layout']['bounding_poly']['normalized_vertices']:\n",
    "            vertices.append(dict(x = vertex['x'] * document_image.size[0], y = vertex['y'] * document_image.size[1]))\n",
    "          use_paragraph = True\n",
    "          for t_shape in tables:\n",
    "            p_shape = shapely.geometry.Polygon([(v['x'], v['y']) for v in vertices])\n",
    "            if p_shape.intersects(t_shape): use_paragraph = False\n",
    "          if use_paragraph:\n",
    "            documents.append(\n",
    "                dict(\n",
    "                    page_content = paragraph_txt,\n",
    "                    metadata = dict(\n",
    "                        page = r+1,\n",
    "                        paragraph = p+1,\n",
    "                        vme_id = str(len(documents)),\n",
    "                        filename = source_document.split('/')[-1],\n",
    "                        source_document = source_document\n",
    "                    ),\n",
    "                    extras = dict(\n",
    "                        vertices = vertices\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "    print(f\"Finished processing of {len(documents)} documents\")\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099a286c-5225-4cde-8b31-c606b012aaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an embedding_space\n",
    "# -----------------------------------\n",
    "# Task 8: Let's discuss an Embedding Space\n",
    "Image(\"img/task_9.png\", width=800)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2739573-3d4a-4b9d-b5b6-aba46af30836",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"img/task_9b.png\", width=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cdab3d-e1ee-452e-b8b5-16c849bea07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"img/task_10.png\", width=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b78ac1f-d18d-4923-a4bd-7ec2406f80c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_space(documents):\n",
    "    \"\"\"\n",
    "    Creates an embedding space for a list of documents.\n",
    "\n",
    "    Args:\n",
    "        documents (list): A list of documents.\n",
    "\n",
    "    Returns:\n",
    "        scann.ScannTree: The embedding space.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set the rate limit for processing documents.\n",
    "    rate_limit_minute = 150\n",
    "\n",
    "    # Print an estimated runtime for embedding space creation.\n",
    "    print(f\"The expected run time for embedding space creation is {(len(documents)*4/rate_limit_minute):.2f} minutes\")\n",
    "\n",
    "    # Start a timer to track the processing time.\n",
    "    start = time.time()\n",
    "\n",
    "    # Process each document and extract its embedding.\n",
    "    for d, document in enumerate(documents):\n",
    "        if d % rate_limit_minute == 0:\n",
    "            # Sleep to avoid exceeding the rate limit.\n",
    "            time.sleep(((time.time() - start) % 60) + 10)\n",
    "            start = time.time()\n",
    "\n",
    "        # Get the text from the document.\n",
    "        text = document['page_content']\n",
    "\n",
    "        # Generate an embedding for the text if it exists.\n",
    "        if text:\n",
    "            embed = embedding_model.get_embeddings([text])[0].values\n",
    "        else:\n",
    "            embed = []\n",
    "\n",
    "        # Store the embedding in the document.\n",
    "        documents[d]['embedding'] = embed\n",
    "\n",
    "    # Create an empty index to store the embeddings.\n",
    "    index = np.empty((len(documents), len(documents[0]['embedding'])))\n",
    "\n",
    "    # Check if the embeddings are lists or numpy arrays.\n",
    "    if type(documents[0]['embedding']) == list:\n",
    "        # Fill the index with the embeddings.\n",
    "        for i in range(index.shape[0]):\n",
    "            if documents[i]['page_content']:\n",
    "                index[i] = documents[i]['embedding']\n",
    "\n",
    "    # Normalize the index.\n",
    "    normalized_index = index / np.linalg.norm(index, axis=1)[:, np.newaxis]\n",
    "\n",
    "    # Create a builder for ScaNN.\n",
    "    builder = scann.scann_ops_pybind.builder(\n",
    "        normalized_index,  # index\n",
    "        10,  # num_neighbors\n",
    "        \"dot_product\",  # distance_measure\n",
    "    )\n",
    "\n",
    "    # Build the embedding space.\n",
    "    embedding_space = builder.tree(\n",
    "        num_leaves=index.shape[0],  # num_leaves\n",
    "        num_leaves_to_search=index.shape[0],  # num_leaves_to_search\n",
    "        training_sample_size=index.shape[0]\n",
    "    ).score_ah(\n",
    "        2,\n",
    "        anisotropic_quantization_threshold=0.2\n",
    "    ).reorder(\n",
    "        index.shape[0]\n",
    "    ).build()\n",
    "\n",
    "    # Print a message indicating that the embedding space has been created.\n",
    "    print(\"Finished creating embedding space\")\n",
    "\n",
    "    # Return the embedding space.\n",
    "    return embedding_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc89400-d6b7-48d4-b490-8035f23a3636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an embedding_space\n",
    "# -----------------------------------\n",
    "# Task 9: Let's discuss ScaNN\n",
    "Image(\"img/task_9c.png\", width=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c044e304-44ec-45d3-873e-857086a04ac1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def search_index(embedding_space, query, k):\n",
    "    \"\"\"\n",
    "    Search for the nearest neighbors of a query in an embedding space.\n",
    "\n",
    "    Args:\n",
    "        embedding_space: An embedding space object.\n",
    "        query: A query vector.\n",
    "        k: The number of nearest neighbors to return.\n",
    "\n",
    "    Returns:\n",
    "        A list of (index, distance) pairs, where `index` is the index of a neighbor in the embedding space\n",
    "        and `distance` is the distance between the query and the neighbor.\n",
    "    \"\"\"\n",
    "    query = embedding_model.get_embeddings([query])[0].values\n",
    "    neighbors, distances = embedding_space.search(query, final_num_neighbors=k)\n",
    "    return list(zip(neighbors, distances))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d93b2b-7913-449d-a34e-3d0479b5d1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_context(question, embedding_space):\n",
    "    \"\"\"\n",
    "    Create a context for a question by finding the nearest neighbors in an embedding space and base on this retrieving the original document.\n",
    "\n",
    "    Args:\n",
    "        question: A question string.\n",
    "        embedding_space: An embedding space object.\n",
    "\n",
    "    Returns:\n",
    "        A string containing the context for the question.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the score for the closest match\n",
    "    score = search_index(embedding_space, question, k=1)[0][1]\n",
    "\n",
    "    # Retrieve related documents - the number is based on the distance\n",
    "    # score from the closest match\n",
    "    relevant_documentation = search_index(embedding_space, question, k=1 + 2 * int(10 * (1 - score)))\n",
    "\n",
    "    # Create the context\n",
    "    context = []\n",
    "    for c, doc in enumerate(relevant_documentation):\n",
    "        context.append(f'Context {c + 1}:\\n' + documents[doc[0]]['page_content'])\n",
    "\n",
    "    return \"\\n\".join(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6c4be4-6a7b-49b6-991c-1990359a44b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(question, context, instructions):\n",
    "    \"\"\"\n",
    "    Create a prompt for a question by formatting the context and instructions to the original prompt.\n",
    "\n",
    "    Args:\n",
    "        question: A question string.\n",
    "        context: A string containing the context for the question.\n",
    "        instructions: A list of instruction strings.\n",
    "\n",
    "    Returns:\n",
    "        A string containing the formatted prompt.\n",
    "    \"\"\"\n",
    "\n",
    "    return f\"\"\"\n",
    "        Give a detailed answer to the question using information from the provided contexts.\n",
    "        {context}\n",
    "        Also include these Instructions:\n",
    "        {\" \".join(instructions)}\n",
    "\n",
    "        Question:\n",
    "        {question}\n",
    "\n",
    "        Answer and Explanation:\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7019d26d-3547-466f-8abd-caa77d1ec284",
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_bot(prompt, parameters):\n",
    "    \"\"\"\n",
    "    Generate a response to a question and provides potential sources for the answer.\n",
    "\n",
    "    Args:\n",
    "        prompt: A prompt string.\n",
    "        parameters: A dictionary of parameters to pass to the text generation model.\n",
    "\n",
    "    Returns:\n",
    "        None.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the score for the closest match\n",
    "    score = search_index(embedding_space, prompt, k=1)[0][1]\n",
    "\n",
    "    # Retrieve related documents - the number is based on the distance score from the closest match\n",
    "    relevant_documentation = search_index(embedding_space, prompt, k=1 + 2 * int(10 * (1 - score)))\n",
    "\n",
    "    # Prepare context for the prompt\n",
    "    context = \"\\n\".join(\n",
    "        [f'Context {c + 1}:\\n' + documents[doc[0]]['page_content'] for c, doc in enumerate(relevant_documentation)])\n",
    "\n",
    "    # Construct the prompt\n",
    "    modified_prompt = f\"\"\"\n",
    "        Give a detailed answer to the question using information from the provided contexts.\n",
    "        {context}\n",
    "        Also include these Instructions:\n",
    "        {\". \".join(instructions)}\n",
    "\n",
    "        Question:\n",
    "        {question}\n",
    "\n",
    "        Answer and Explanation:\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate a response\n",
    "    response = textgen_model.predict(modified_prompt, **parameters)\n",
    "\n",
    "    # Get the closest document to the response\n",
    "    likely_source = search_index(embedding_space, response.text, k=1)[0]\n",
    "\n",
    "    # Declare the likely source: if the closest document to the response was in the context, pick it, otherwise pick the first context match\n",
    "    if likely_source[0] in [rd[0] for rd in relevant_documentation]:\n",
    "        likely = True\n",
    "        likely_document = documents[likely_source[0]]\n",
    "    else:\n",
    "        likely = False\n",
    "        likely_document = documents[relevant_documentation[0][0]]\n",
    "\n",
    "    # Format the sources\n",
    "    sources = \"\\n\".join(\n",
    "        f\"* {documents[doc[0]]['metadata']['source_document']}#page={documents[doc[0]]['metadata']['page']}\\n\\t* Document: {documents[doc[0]]['metadata']['filename']}, page: {documents[doc[0]]['metadata']['page']}, relevance to question: {doc[1]:.2f}\"\n",
    "        for doc in relevant_documentation)\n",
    "\n",
    "    # Format the answer\n",
    "    answer = f\"\"\"---\n",
    "**Search results** \n",
    "Score: {score} \n",
    "Closest document: {likely_source} \n",
    "\"\"\"\n",
    "\n",
    "    if likely:\n",
    "        answer = answer + f\"Document used to generate response: {likely_document['metadata']['filename']}, page: {likely_document['metadata']['page']}\"\n",
    "    else:\n",
    "        answer = answer + f\"First relevant document used to generate response: {likely_document['metadata']['filename']}, page: {likely_document['metadata']['page']}\"\n",
    "\n",
    "    answer = answer + f\"\"\"\n",
    "\n",
    "\n",
    "{question}\n",
    "\n",
    "**Answer**\n",
    "{response.text}\n",
    "\n",
    "**Potential Sources**  \n",
    "{sources}\n",
    "    \"\"\"\n",
    "\n",
    "    # Display the answer\n",
    "    IPython.display.display(IPython.display.Markdown(answer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0c71ad-d942-4769-acd4-1e0fd871a7fa",
   "metadata": {},
   "source": [
    "# Extract text from PDF document in GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75de19ce-8ffb-4baf-a48f-2422ff53c912",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "documents = extract_pdf_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf81b38-9f31-4e3e-8eca-9d1c2c87c74b",
   "metadata": {},
   "source": [
    "# Create embedding space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e731a7-0872-4168-b047-e1b9183a6600",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedding_space = create_embedding_space(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9f122e-f720-4d0f-b9dd-d747a8ccc24a",
   "metadata": {},
   "source": [
    "# Define a Help Center question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e417519-f48b-4e35-9e0a-f1eb0645c9c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"What is the refund policy?\"\n",
    "\n",
    "# Example Questions:\n",
    "#question = \"What are essential amenities for guests?\"\n",
    "#question = \"How much is the minimum cancellation fee for Hosts on Cymbal for a confirmed reservation?\"\n",
    "#question = \"How old is Barack Obama?\"\n",
    "#question = \"Is it a nice time travelling to Sydney in June?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc449167-3818-4657-bc01-15aa450cb838",
   "metadata": {},
   "source": [
    "# Create context to be used in LLM prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770c73e3-21da-4e9b-95ff-061803370a1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "context = create_context(question, embedding_space)\n",
    "context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da0828b-d16d-473e-82a6-304e3f9a13d6",
   "metadata": {},
   "source": [
    "# Create instructions to be used in LLM prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83a0051-35da-455b-9789-48c30c35afbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "instructions = [\n",
    "    \"If the question can be answered by the context provided above alone than only use the context to provide the answer. However, never use other information than from the context above to formulate the answer\",\n",
    "    \"If the question cannot be answered by the context provided above but is a travel question then use the underlying LLM to answer. In particular state: the context above is not sufficient to answer the question but this is what I found in the base-model\"\n",
    "]\n",
    "\". \".join(instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b246d6-9471-49d9-a0a2-3ff237b2cbd1",
   "metadata": {},
   "source": [
    "# Create LLM prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d948e55-7251-4670-aefd-946de1e3d844",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = create_prompt(question, context, instructions)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff2dde8-9e35-450c-bb0f-31efaa797414",
   "metadata": {},
   "source": [
    "# Define prediction parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcf3e0c-a9fd-4960-aa1b-03cd52a85380",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"top_k\": 40,  # A top_k of 1 means the selected token is the most probable among all tokens.\n",
    "    \"top_p\": 0.8,  # Tokens are selected from most probable to least until the sum of their probabilities equals the top_p value.\n",
    "    \"temperature\": 0.7,  # Temperature controls the degree of randomness in token selection.\n",
    "    \"max_output_tokens\": 1000,  # Token limit determines the maximum amount of text output.\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88841f70-6029-4ef0-87e3-8c3db64deef4",
   "metadata": {},
   "source": [
    "# Ask the Help Center question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10aae1b-d646-4e13-80be-68a485fde8dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "document_bot(prompt, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c536fd5-61ec-49fa-8e13-6f913676d426",
   "metadata": {},
   "source": [
    "# Ask a travel question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bbacfd-b016-4b1e-95cd-a8fcd821945c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"Is it a nice time to travel to Sydney in June?\"\n",
    "context = create_context(question, embedding_space)\n",
    "prompt = create_prompt(question, context, instructions)\n",
    "document_bot(prompt, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20239e0d-3784-45f9-914f-fc30e82561c2",
   "metadata": {},
   "source": [
    "# Ask a non-travel question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb101515-8245-4a5d-8b7c-982e66022fee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"Who is Barack Obama?\"\n",
    "context = create_context(question, embedding_space)\n",
    "prompt = create_prompt(question, context, instructions)\n",
    "document_bot(prompt, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0626d65d-5982-4bec-aa85-39ad7aaa022a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Change instructions to restrict to travel questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892928ad-73f8-4696-b3fe-74e3aaa5dd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = [\n",
    "    \"If the question can be answered by the context provided above alone than only use the context to provide the answer. However, never use other information than from the context above to formulate the answer\",\n",
    "    \"If the prompt does not contain a travel specific question then do not contiune. Instead end the conversation with: This is not a travel question, sorry I cannot answer this\",\n",
    "    \"If the question cannot be answered by the context provided above but is a travel question then use the underlying LLM to answer. In particular state: the context above is not sufficient to answer the question but this is what I found in the base-model\"\n",
    "]\n",
    "\". \".join(instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a063dc-a852-4541-b8d7-f7b2447e3835",
   "metadata": {},
   "source": [
    "# Ask the non-travel related question again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1e3784-5a84-4872-88ba-7044a292f069",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"Who is Barack Obama?\"\n",
    "context = create_context(question, embedding_space)\n",
    "prompt = create_prompt(question, context, instructions)\n",
    "document_bot(prompt, parameters)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-13.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-13:m112"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
